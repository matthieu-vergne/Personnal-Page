<!DOCTYPE html>
<html lang="en">
<head>
	<title>Blog Page</title>
	
	<meta charset="utf-8">
	
	<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/themes/prism-okaidia.css" rel="stylesheet" />
	<link href="../style.css" rel="stylesheet" type="text/css" />
	
	<script type="text/x-mathjax-config">
		MathJax.Hub.Config({
			TeX: {
				extensions: ["color.js"],
				equationNumbers: { autoNumber: "AMS" },
			}
		});
	</script>
	<script language="JavaScript" type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/components/prism-core.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/plugins/autoloader/prism-autoloader.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/plugins/keep-markup/prism-keep-markup.min.js"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.17.1/plugins/normalize-whitespace/prism-normalize-whitespace.min.js"></script>
	<script language="JavaScript" type="text/javascript" src="blogEdit.js"></script>
</head>
<body id="blog" class="language-java">
<div class="language-bash">
<h1>How to Review your Code?</h1>

<h2 id="context">Context</h2>

<p>
	Programming can be done in various contexts, en depending on it you may spend various amounts of effort into it.
	For a simple proof of concept (PoC), you can produce some dirty code as long as it shows the expected behaviour.
	But when you target a long term support, the code must be produced with care.
	Of course, there is a spectrum between both, and you have to balance it.
</p>
<p>
	We often speak about technical solutions, like automated tests (unit, integration, system, etc.) or linters (formatter, smells detector, etc.).
	However, technical solutions must be chosen, installed, configured, and actually used to have an effect.
	Some of them might be pro-active, like formatting upon saving your changes, but only if we enable them and as long as we don't change our mind.
	At the end, technical solutions only support our own, human reviews of the code.
</p>
<p>
	Reviewing your code is thus a fundamental practice in programming.
	Whether or not you use automated tools, at the end you must ensure that your code fulfill some requirements.
	And these requirements must be carefully chosen based on the context.
</p>
<p>
	Reviewing your code can come in two flavours: reviewing code produced by you or by someone else.
	Let's assume that the code of the others becomes also your own as soon as you review it because you are the one giving the go.
	Of course, it is not that simple, but this is the short way to say that you should care as mush about both.
	Consequently, we make no difference between both flavours here.
</p>

<h2 id="question">Question</h2>

<p>
	Various criteria can be considered during the review.
	That means that a lof of questions can be considered:
</p>
<ul>
	<li>What are the relevant features to implement?</li>
	<li>Is each feature correctly tested?</li>
	<li>Is the code correctly designed?</li>
	<li>Is it clear enough for your colleagues to work on it?</li>
	<li>Does it follow the team conventions?</li>
	<li>Etc.</li>
</ul>
<p>
	Some of them are generic (having relevant features), others are context-dependent (following team conventions).
	Some of them are obvious (being tested), others are vague (having a correct design).
	Some of them are high level (being clear), others are specializations of them (following team conventions).
	We can dig as long as we want into all the relevant questions that might be considered, but there is no end to it.
	Instead, let's focus on their common, fundamental aspect:
</p>
<ul>
	<li>How to review your code?</li>
</ul>

<h2 id="method">Method</h2>

<p>
	TODO
</p>

<h3 id="method-goals">Goals of the Code Review</h3>

<p>
	One of the goals when reviewing code is to give a second eye to decrease the risks of mistakes.
	Spending more effort on the review decreases further the risk, but also the time available for doing something else.
	Thus, there is a balance to find.
	Automated tools help to improve this balance by moving some of the effort on the machine.
	If you find some tools that cover parts of the review where you spend a significant amount of time, consider using them.
	Try to optimise this balance based on your context.
</p>
<p>
	Another goal of the review is knowledge transfer.
	It works in both ways: an expert can share by reviewing the code of a novice, a novice can learn by reviewing the code of an expert.
	The expert review comes mainly with suggestions for improvements, while the novice review comes mainly with questions to understand.
	However, it often happens that a comment or question arises when reading some code, before to take it back by reading other pieces of code.
	Indeed, the code has its own context: it should be reviewed in the right order to have all the relevant information.
</p>
<p>
	In summary, when reviewing your code, you have to prioritise your review: focus on the most fundamental parts before to go on what builds on them.
	Regarding the risk of mistakes, they can have more impact since the rest builds on them.
	Regarding the knowledge transfer, they provide the initial information required to interpret the rest.
	As soon as the critical parts are good, you can go ahead.
</p>
<p>
	Now is the main issue: how do we identify the fundamental parts?
</p>

<h3 id="method-theory">Theory</h3>

<p>
	To prioritise your review, you may take a technical perspective.
	Using a Java example, you may review a base class before to review its extensions.
	When you know what a base class does, you can understand better what the extensions add and override.
	And when you master completely the hierarchy of classes, you may look at their usage to understand how they are combined.
</p>
<p>
	Although it seems intuitive, this is actually a poor way to go.
	Indeed, you read first <em>how</em> it is done without knowing <em>why</em> it should be that way.
	Only when you reach the usage, it starts making sense.
	Or does it?
	Usually, this is when you start figuring out that what you understood was not (exactly) what was intended.
	Maybe you even started doubting it a while ago, but you didn't have the relevant information yet to confirm your doubts.
	Actually, that was the ringing bell telling you that you were not looking at the right thing.
	But it is already too late: you have spent a lot of time reviewing something that must be changed because it does not fulfill the actual needs.
</p>
<p>
	The priority must be reversed: focus on the <em>why</em> before to dig in the <em>how</em>.
	Actually, this is exactly the same than coding: before to write your code, you must have a good understanding of the problem you want to solve.
	In practice, however, we often go iteratively because we discover the various aspects of the problem on the go.
	But if done correctly, the code should evolve towards a finished state, as if it was produced with a perfect preparation.
	The review assesses exactly that: assuming we had prepared perfectly, would we have the same code?
	The review process is thus following the same steps than the ideal coding process.
</p>
<p>
	At this point, we must tell what is an "ideal" coding process.
	But as any "ideal" thing, this is a matter of perspective.
	I want to target a good trade-off between relevance and simplicity.
	So here is the guideline I suggest:
</p>
<ol>
	<li>Make it work.</li>
	<li>Make it right.</li>
	<li>Make it fast.</li>
</ol>
<p>
	These three steps are a known good practice attributed to <a href="https://en.wikipedia.org/wiki/Kent_Beck">Kent Beck</a>.
	It has been running long enough to build interpretations and variants.
	<a href="https://tknilsson.com/2018/05/25/programmer-friday-make-it-work-make-it-right-make-it-fast/">Some</a> interpret it as starting "quick and dirty", then testing and making the code maintainable, before to improve the performances if time allows it.
	<a href="https://henriquebastos.net/the-make-it-work-make-it-right-make-it-fast-misconception/">Others</a> consider it as a checklist that must be completed before any delivery.
	Others again extend its interpretation, like generalising to an <a href="https://thetombomb.com/posts/make-it-work-right-fast">increase of value</a> rather than performance.
</p>
<p>
	Here, we use these three steps as a general guideline.
	They can be applied at system level: starting with a PoC, then we refactor progressively using design patterns and adding automated tests, before to adapt the performances based on some benchmarks.
	If we wait too long before refactoring and testing, we may reach a state that no one, not even you, can figure out how to improve without breaking.
	Consequently, it makes also sense to apply them at feature level: create the feature, make it clean and fast before to integrate it in the rest of the code.
	If implemented too independently, we may miss the relevant information to identify the right design and performance trade-offs, leading to <a href="https://en.wikipedia.org/wiki/Program_optimization#When_to_optimize">premature optimisation</a>.
	Nevertheless, we can also apply them at finer levels, like a method: create the method, clean its code and optimise it.
	As any scale, truth is somewhere between both extremes, just remember that it is a generic guideline applicable to any level and use it wisely.
</p>
<p>
	But this is about <em>coding</em>, what about the <em>review</em>?
	The review process is following the same steps than the ideal coding process.
	So when reviewing your code, just adapt these steps to a review process:
</p>
<ol>
	<li>Does the code work?</li>
	<li>Is the code right?</li>
	<li>Is the code fast?</li>
</ol>
<p>
	These are the fundamental questions you should answer, in order, during the review.
</p>

<h3 id="method-practice">Practice</h3>

<p>
	Once we get the overall idea, let's apply it:
</p>
<table>
	<tr>
		<th>Step</th>
		<th>Goal</th>
		<th>Interpretation</th>
		<th>To be done...</th>
		<th>Reason</th>
		<th>Content to review</th>
	</tr>
	<tr>
		<td>1</td>
		<td>Make it work.</td>
		<td>Ensure it does what it is supposed to.</td>
		<td>Always, unless motivated.</td>
		<td>This is what has been requested.</td>
		<td>Tests (manual, auto) and code which passes the tests.</td>
	</tr>
	<tr>
		<td>2</td>
		<td>Make it right.</td>
		<td>Apply local conventions and good practices.</td>
		<td>As much as intended.</td>
		<td>Ease maintenance and evolution.</td>
		<td>Documentation, full test coverage, design patterns, good practices (SOLID, KISS, etc.).</td>
	</tr>
	<tr>
		<td>3</td>
		<td>Make it fast.</td>
		<td>Break some practices for better trade-offs.</td>
		<td>Never, unless motivated.</td>
		<td>Avoid premature optimisations.</td>
		<td>Benchmarks showing trade-offs, clarifying comments, etc.</td>
	</tr>
</table>
<p>
	It might be surprising to some people that the last step is to be avoided, unless motivated.
	It might be interpreted as avoiding any performance improvement without having some extensive benchmarks.
	This is of course not the case, but this interpretation lies on the ambiguity behind the notion of <entry id="32">optimisation</entry>.
	This is a long term task which builds on human evaluations and available metrics.
	Step 2 builds mainly on human evaluations, which is always available and allow to evaluate the <em>semantic</em> of the code (making it right).
	Step 3 builds mainly on available metrics, which require time to be implemented and can be hard to evaluate reliably the <em>performance</em> (making it fast).
	Both may improve the performance, but step 2 obtains it as a secondary consequence, while step 3 seeks it through trade-offs with step 2.
	Once we understand that step 3 is about optimising by trade-off, it means losing something to gain something else.
	At this point, it becomes obvious that it must be motivated: we should clearly understand what we lose and why it weights less than what we gain.
</p>
<p>
	In the end, a well balanced code is delivered somewhere in step 2.
	If comments arise because of step 1, it means the code might not work, so it cannot be delivered.
	If comments arise because of step 3, it means trade-offs have been made, so we should spend extra effort motivating them.
	Often, benchmarks and other profiling efforts occur after delivery, when problems or limits are actually observed.
	This is the right time to spend this extra effort on actual issues and where step 3 comes naturally.
</p>

<h2 id="answer">Answer</h2>

<p>
	TODO
</p>

<h2 id="links">Related Questions</h2>

<ul>
	<li><entry id="25"/></li>
</ul>

<h2 id="bibliography">Bibliography</h2>

<ul>
<li id="gitbook"><cite>Git Book</cite>: <a href="https://git-scm.com/book/en/v2">https://git-scm.com/book/en/v2</a></li>
</ul>

</div>
</body>
</html>
